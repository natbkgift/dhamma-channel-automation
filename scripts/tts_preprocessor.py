"""
TTS Text Preprocessor Agent
‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏á AI ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ò‡∏£‡∏£‡∏°‡∏∞
"""

import re
from pathlib import Path
from typing import Dict, List, Tuple


class TTSPreprocessor:
    """
    Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥ TTS
    - ‡∏•‡∏ö‡πÅ‡∏ó‡∏Å markdown, HTML
    - ‡∏•‡∏ö‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©
    - ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô
    """
    
    def __init__(self):
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏ö‡∏≠‡∏≠‡∏Å
        self.removal_patterns = [
            # Markdown headers
            (r'^#{1,6}\s+', ''),
            
            # Markdown bold/italic
            (r'\*\*(.+?)\*\*', r'\1'),  # **bold** ‚Üí bold
            (r'\*(.+?)\*', r'\1'),      # *italic* ‚Üí italic
            (r'__(.+?)__', r'\1'),      # __bold__ ‚Üí bold
            (r'_(.+?)_', r'\1'),        # _italic_ ‚Üí italic
            
            # Markdown links
            (r'\[(.+?)\]\(.+?\)', r'\1'),  # [text](url) ‚Üí text
            
            # HTML tags
            (r'<[^>]+>', ''),
            
            # Hashtags (‡πÅ‡∏ó‡∏Å)
            (r'#\w+', ''),
            
            # Mentions
            (r'@\w+', ''),
            
            # URLs
            (r'https?://\S+', ''),
            (r'www\.\S+', ''),
            
            # Emojis (Unicode ranges)
            (r'[\U0001F600-\U0001F64F]', ''),  # Emoticons
            (r'[\U0001F300-\U0001F5FF]', ''),  # Symbols & Pictographs
            (r'[\U0001F680-\U0001F6FF]', ''),  # Transport & Map
            (r'[\U0001F1E0-\U0001F1FF]', ''),  # Flags
            (r'[\U00002702-\U000027B0]', ''),  # Dingbats
            (r'[\U000024C2-\U0001F251]', ''),  # Enclosed characters
            
            # ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô
            (r'[‚òÖ‚òÜ‚úì‚úî‚úó‚úò‚ñ∫‚ñ∂‚óÑ‚óÄ‚ñ≤‚ñº‚óè‚óã‚ñ†‚ñ°]', ''),
            
            # ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå
            (r'[=]{2,}', ''),  # ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏ã‡πâ‡∏≥‡πÜ ==== ‚Üí ‡∏•‡∏ö
            (r'[-]{3,}', ''),  # ‡∏Ç‡∏µ‡∏î‡∏ã‡πâ‡∏≥‡πÜ ---- ‚Üí ‡∏•‡∏ö
            (r'[_]{3,}', ''),  # ‡∏Ç‡∏µ‡∏î‡∏•‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÜ ____ ‚Üí ‡∏•‡∏ö
            (r'[~]{2,}', ''),  # tilde ‡∏ã‡πâ‡∏≥‡πÜ ~~~~ ‚Üí ‡∏•‡∏ö
            (r'[+]{2,}', ''),  # ‡∏ö‡∏ß‡∏Å‡∏ã‡πâ‡∏≥‡πÜ ++++ ‚Üí ‡∏•‡∏ö
            
            # Separator lines (‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á)
            (r'^[=\-_~+]{3,}$', '', re.MULTILINE),
            
            # Bullet points ‡πÅ‡∏•‡∏∞ list markers
            (r'^\s*[-‚Ä¢¬∑]\s+', '', re.MULTILINE),
            (r'^\s*\d+\.\s+', '', re.MULTILINE),  # 1. 2. 3.
            
            # ‡∏•‡∏ö‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡πÄ‡∏õ‡∏•‡πà‡∏≤
            (r'\(\s*\)', ''),
            (r'\[\s*\]', ''),
        ]
        
        # ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢
        self.abbreviations = {
            # ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            '‡∏°.': '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢',
            '‡∏î‡∏£.': '‡∏î‡∏≠‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå',
            '‡∏ú‡∏®.': '‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå',
            '‡∏£‡∏®.': '‡∏£‡∏≠‡∏á‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå',
            '‡∏®.': '‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå',
            '‡∏û.‡∏®.': '‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏±‡∏Å‡∏£‡∏≤‡∏ä',
            '‡∏Ñ.‡∏®.': '‡∏Ñ‡∏£‡∏¥‡∏™‡∏ï‡πå‡∏®‡∏±‡∏Å‡∏£‡∏≤‡∏ä',
            '‡∏≠.': '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',
            '‡∏à.': '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',
            '‡∏ï.': '‡∏ï‡∏≥‡∏ö‡∏•',
            
            # ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
            'Mr.': 'Mister',
            'Mrs.': 'Missis',
            'Dr.': 'Doctor',
            'Prof.': 'Professor',
        }
        
        # ‡∏Ñ‡∏≥‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        self.dhamma_terms = {
            '‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡πÄ‡∏à‡πâ‡∏≤': '‡∏û‡∏£‡∏∞-‡∏û‡∏∏‡∏ó‡∏ò-‡πÄ‡∏à‡πâ‡∏≤',
            '‡∏û‡∏£‡∏∞‡∏≠‡∏£‡∏¥‡∏¢‡πÄ‡∏à‡πâ‡∏≤': '‡∏û‡∏£‡∏∞-‡∏≠‡∏∞‡∏£‡∏¥‡∏¢‡∏∞-‡πÄ‡∏à‡πâ‡∏≤',
            '‡∏°‡∏±‡∏ä‡∏å‡∏¥‡∏°‡∏≤‡∏õ‡∏è‡∏¥‡∏õ‡∏ó‡∏≤': '‡∏°‡∏±‡∏î-‡∏å‡∏¥-‡∏°‡∏≤-‡∏õ‡∏∞‡∏è‡∏¥-‡∏õ‡∏∞‡∏ó‡∏≤',
            '‡∏≠‡∏ô‡∏≤‡∏õ‡∏≤‡∏ô‡∏™‡∏ï‡∏¥': '‡∏≠‡∏∞‡∏ô‡∏≤-‡∏õ‡∏≤-‡∏ô‡∏∞-‡∏™‡∏∞‡∏ï‡∏¥',
            '‡∏ß‡∏¥‡∏õ‡∏±‡∏™‡∏™‡∏ô‡∏≤': '‡∏ß‡∏¥-‡∏õ‡∏±‡∏î-‡∏™‡∏∞‡∏ô‡∏≤',
            '‡∏™‡∏°‡∏ñ‡∏∞': '‡∏™‡∏∞‡∏°‡∏∞‡∏ó‡∏∞',
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        }
    
    def preprocess(self, text: str) -> Tuple[str, Dict[str, any]]:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        Returns:
            Tuple[str, Dict]: (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß, metadata)
        """
        original_length = len(text)
        metadata = {
            'original_length': original_length,
            'original_bytes': len(text.encode('utf-8')),
            'changes': []
        }
        
        # 1. ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô
        text, removed = self._remove_non_speech_content(text)
        if removed:
            metadata['changes'].append(f"Removed: {', '.join(removed)}")
        
        # 2. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
        text = self._clean_formatting(text)
        metadata['changes'].append("Cleaned formatting")
        
        # 3. ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠
        text, expanded = self._expand_abbreviations(text)
        if expanded:
            metadata['changes'].append(f"Expanded {len(expanded)} abbreviations")
        
        # 4. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        text = self._process_numbers(text)
        metadata['changes'].append("Processed numbers")
        
        # 5. ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô
        text = self._fix_punctuation(text)
        metadata['changes'].append("Fixed punctuation")
        
        # 6. ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô
        text = self._clean_whitespace(text)
        
        # 7. ‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏•‡∏î max ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 70 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Google TTS ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏°‡∏≤‡∏Å)
        text = self._split_long_sentences(text, max_length=70)
        
        metadata['final_length'] = len(text)
        metadata['final_bytes'] = len(text.encode('utf-8'))
        metadata['reduction'] = f"{(1 - len(text)/original_length)*100:.1f}%"
        
        return text, metadata
    
    def _remove_non_speech_content(self, text: str) -> Tuple[str, List[str]]:
        """‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á"""
        removed = []
        
        # ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô metadata (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ emoji ‡∏´‡∏£‡∏∑‡∏≠ symbols)
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á
            if not line:
                cleaned_lines.append('')
                continue
            
            # ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ó‡∏Å (‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ #)
            if line.startswith('#') and ' ' not in line[:20]:
                removed.append('hashtags')
                continue
            
            # ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô metadata (‡∏°‡∏µ emoji ‡∏´‡∏£‡∏∑‡∏≠ :: ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)
            if re.match(r'^[\U0001F300-\U0001F9FF]', line):
                removed.append('emoji lines')
                continue
            
            if line.startswith('::'):
                removed.append('metadata')
                continue
            
            # ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏•‡πâ‡∏ß‡∏ô‡πÜ
            if line.startswith('http') or line.startswith('www.'):
                removed.append('urls')
                continue
            
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines), list(set(removed))
    
    def _clean_formatting(self, text: str) -> str:
        """‡∏•‡∏ö markdown ‡πÅ‡∏•‡∏∞ formatting ‡∏ï‡πà‡∏≤‡∏á‡πÜ"""
        for pattern, replacement, *flags in self.removal_patterns:
            flag = flags[0] if flags else 0
            text = re.sub(pattern, replacement, text, flags=flag)
        
        return text
    
    def _expand_abbreviations(self, text: str) -> Tuple[str, List[str]]:
        """‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠"""
        expanded = []
        
        for abbr, full in self.abbreviations.items():
            if abbr in text:
                text = text.replace(abbr, full)
                expanded.append(abbr)
        
        return text, expanded
    
    def _process_numbers(self, text: str) -> str:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏á‡πà‡∏≤‡∏¢‡πÜ)"""
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢
        thai_numbers = {
            '0': '‡∏®‡∏π‡∏ô‡∏¢‡πå', '1': '‡∏´‡∏ô‡∏∂‡πà‡∏á', '2': '‡∏™‡∏≠‡∏á', '3': '‡∏™‡∏≤‡∏°', '4': '‡∏™‡∏µ‡πà',
            '5': '‡∏´‡πâ‡∏≤', '6': '‡∏´‡∏Å', '7': '‡πÄ‡∏à‡πá‡∏î', '8': '‡πÅ‡∏õ‡∏î', '9': '‡πÄ‡∏Å‡πâ‡∏≤',
            '10': '‡∏™‡∏¥‡∏ö', '20': '‡∏¢‡∏µ‡πà‡∏™‡∏¥‡∏ö', '30': '‡∏™‡∏≤‡∏°‡∏™‡∏¥‡∏ö', '100': '‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡πâ‡∏≠‡∏¢',
        }
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢ (1-100)
        def replace_simple_number(match):
            num = int(match.group(0))
            if num in range(1, 11):
                return ['‡∏´‡∏ô‡∏∂‡πà‡∏á', '‡∏™‡∏≠‡∏á', '‡∏™‡∏≤‡∏°', '‡∏™‡∏µ‡πà', '‡∏´‡πâ‡∏≤', '‡∏´‡∏Å', '‡πÄ‡∏à‡πá‡∏î', '‡πÅ‡∏õ‡∏î', '‡πÄ‡∏Å‡πâ‡∏≤', '‡∏™‡∏¥‡∏ö'][num-1]
            elif num == 20:
                return '‡∏¢‡∏µ‡πà‡∏™‡∏¥‡∏ö'
            elif num < 100:
                tens = num // 10
                ones = num % 10
                result = ['', '‡∏™‡∏¥‡∏ö', '‡∏¢‡∏µ‡πà‡∏™‡∏¥‡∏ö', '‡∏™‡∏≤‡∏°‡∏™‡∏¥‡∏ö', '‡∏™‡∏µ‡πà‡∏™‡∏¥‡∏ö', '‡∏´‡πâ‡∏≤‡∏™‡∏¥‡∏ö', '‡∏´‡∏Å‡∏™‡∏¥‡∏ö', '‡πÄ‡∏à‡πá‡∏î‡∏™‡∏¥‡∏ö', '‡πÅ‡∏õ‡∏î‡∏™‡∏¥‡∏ö', '‡πÄ‡∏Å‡πâ‡∏≤‡∏™‡∏¥‡∏ö'][tens]
                if ones > 0:
                    if ones == 1 and tens > 0:
                        result += '‡πÄ‡∏≠‡πá‡∏î'
                    else:
                        result += ['', '‡∏´‡∏ô‡∏∂‡πà‡∏á', '‡∏™‡∏≠‡∏á', '‡∏™‡∏≤‡∏°', '‡∏™‡∏µ‡πà', '‡∏´‡πâ‡∏≤', '‡∏´‡∏Å', '‡πÄ‡∏à‡πá‡∏î', '‡πÅ‡∏õ‡∏î', '‡πÄ‡∏Å‡πâ‡∏≤'][ones]
                return result
            return match.group(0)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç standalone (‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
        text = re.sub(r'\b(\d{1,2})\b', replace_simple_number, text)
        
        # ‡πÅ‡∏õ‡∏•‡∏á percentages
        text = re.sub(r'(\d+)%', r'\1 ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå', text)
        
        return text
    
    def _fix_punctuation(self, text: str) -> str:
        """‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô"""
        
        # ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ã‡πâ‡∏≥‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        text = re.sub(r'[=]{2,}', ' ', text)  # ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏ã‡πâ‡∏≥‡πÜ
        text = re.sub(r'[-]{3,}', ' ', text)  # ‡∏Ç‡∏µ‡∏î‡∏ã‡πâ‡∏≥‡πÜ
        text = re.sub(r'[_]{3,}', ' ', text)  # ‡∏Ç‡∏µ‡∏î‡∏•‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÜ
        text = re.sub(r'[~]{2,}', ' ', text)  # tilde ‡∏ã‡πâ‡∏≥‡πÜ
        text = re.sub(r'[+]{2,}', ' ', text)  # ‡∏ö‡∏ß‡∏Å‡∏ã‡πâ‡∏≥‡πÜ
        text = re.sub(r'[*]{2,}', ' ', text)  # ‡∏î‡∏≠‡∏Å‡∏à‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÜ
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° space ‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô
        text = re.sub(r'([,.!?;:])([^\s])', r'\1 \2', text)
        
        # ‡∏•‡∏ö multiple punctuation
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        text = re.sub(r'[.]{2,}', '...', text)  # ... ‡∏Ñ‡∏∑‡∏≠ ellipsis
        
        # ‡πÅ‡∏õ‡∏•‡∏á ... ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î (AI ‡∏≠‡πà‡∏≤‡∏ô ... ‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏î‡∏µ)
        text = re.sub(r'\.{3,}', '.', text)
        
        # ‡∏•‡∏ö punctuation ‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ,. ‡∏´‡∏£‡∏∑‡∏≠ ;,
        text = re.sub(r'[,;]\s*[,;.!?]', '.', text)
        
        # ‡πÅ‡∏õ‡∏•‡∏á : ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î
        text = re.sub(r':\s*(?!\d)', '. ', text)
        
        return text
    
    def _clean_whitespace(self, text: str) -> str:
        """‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô"""
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏ï‡∏±‡∏ß
        text = re.sub(r' +', ' ', text)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ï‡πâ‡∏ô/‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
        text = '\n'.join(line.strip() for line in text.split('\n'))
        
        # ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô
        text = re.sub(r'\s+([,.!?;:])', r'\1', text)
        
        return text.strip()
    
    def _split_long_sentences(self, text: str, max_length: int = 70) -> str:
        """‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÅ‡∏¢‡∏Å‡∏ó‡∏∏‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ max_length"""
        
        lines = text.split('\n')
        result = []
        
        for line in lines:
            line = line.strip()
            if not line:
                result.append('')
                continue
            
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô [PAUSE] ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
            if line.startswith('[PAUSE'):
                result.append(line)
                continue
            
            # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡∏±‡∏î‡∏ó‡∏∏‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ max_length
            while len(line) > max_length:
                # ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ó‡∏µ‡πà‡∏°‡∏µ space, comma, period)
                cut_pos = max_length
                
                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ . ‡∏Å‡πà‡∏≠‡∏ô
                last_period = line[:max_length].rfind('.')
                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ space
                last_space = line[:max_length].rfind(' ')
                # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ comma
                last_comma = line[:max_length].rfind(',')
                
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                cut_pos = max(last_period, last_space, last_comma)
                if cut_pos <= 0:
                    cut_pos = max_length
                
                # ‡∏ï‡∏±‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ result
                part = line[:cut_pos+1].strip()
                if part:
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏∏‡∏î‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
                    if not part.endswith(('.', '!', '?', ']')):
                        part += '.'
                    result.append(part)
                
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
                line = line[cut_pos+1:].strip()
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            if line:
                if not line.endswith(('.', '!', '?', ']')):
                    line += '.'
                result.append(line)
        
        return '\n'.join(result)
    
    def _split_by_punctuation(self, text: str, max_length: int, result: list):
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô - DEPRECATED ‡πÉ‡∏ä‡πâ _split_long_sentences ‡πÅ‡∏ó‡∏ô"""
        pass
    
    def analyze_text(self, text: str) -> Dict[str, any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"""
        
        analysis = {
            'total_chars': len(text),
            'total_bytes': len(text.encode('utf-8')),
            'total_words': len(text.split()),
            'total_lines': len(text.split('\n')),
            'has_hashtags': bool(re.search(r'#\w+', text)),
            'has_urls': bool(re.search(r'https?://\S+', text)),
            'has_emojis': bool(re.search(r'[\U0001F300-\U0001F9FF]', text)),
            'has_markdown': bool(re.search(r'\*\*|\*|__|_|#{1,6}\s', text)),
            'sentences': len(re.findall(r'[.!?]+', text)),
        }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏¢‡∏≤‡∏ß
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
        long_sentences = [s for s in sentences if len(s) > 150]
        
        analysis['long_sentences'] = len(long_sentences)
        if long_sentences:
            analysis['longest_sentence_chars'] = max(len(s) for s in long_sentences)
        
        return analysis


def process_file(input_path: Path, output_path: Path = None, verbose: bool = True) -> Dict:
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå
    
    Args:
        input_path: ‡πÑ‡∏ü‡∏•‡πå input
        output_path: ‡πÑ‡∏ü‡∏•‡πå output (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡∏ö input)
        verbose: ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    
    Returns:
        Dict: metadata ‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    """
    
    preprocessor = TTSPreprocessor()
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
    with open(input_path, 'r', encoding='utf-8') as f:
        original_text = f.read()
    
    if verbose:
        print(f"üìñ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {input_path}")
        print(f"üìä ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö: {len(original_text):,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    if verbose:
        print("\nüîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö:")
        analysis = preprocessor.analyze_text(original_text)
        for key, value in analysis.items():
            if value and value is not False:
                print(f"   ‚Ä¢ {key}: {value}")
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    if verbose:
        print("\n‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...")
    
    cleaned_text, metadata = preprocessor.preprocess(original_text)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
    if verbose:
        print(f"\n‚ú® ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:")
        print(f"   ‚Ä¢ ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏î‡∏¥‡∏°: {metadata['original_length']:,} chars ({metadata['original_bytes']:,} bytes)")
        print(f"   ‚Ä¢ ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏°‡πà: {metadata['final_length']:,} chars ({metadata['final_bytes']:,} bytes)")
        print(f"   ‚Ä¢ ‡∏•‡∏î‡∏•‡∏á: {metadata['reduction']}")
        print(f"   ‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á:")
        for change in metadata['changes']:
            print(f"     - {change}")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
    output_path = output_path or input_path.parent / f"{input_path.stem}_cleaned{input_path.suffix}"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(cleaned_text)
    
    if verbose:
        print(f"\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {output_path}")
    
    metadata['input_file'] = str(input_path)
    metadata['output_file'] = str(output_path)
    
    return metadata


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö TTS')
    parser.add_argument('--input', '-i', type=str, required=True,
                       help='‡πÑ‡∏ü‡∏•‡πå input')
    parser.add_argument('--output', '-o', type=str, default=None,
                       help='‡πÑ‡∏ü‡∏•‡πå output (default: {input}_cleaned.txt)')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î')
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    output_path = Path(args.output) if args.output else None
    
    if not input_path.exists():
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {input_path}")
        exit(1)
    
    metadata = process_file(input_path, output_path, verbose=not args.quiet)
    
    print(f"\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
